{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 01 - Core Python\n",
    "Tyler Bevan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello,  world!'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Page 11\n",
    "\"    Hello,  world!  \\t\\t\\n\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello,', 'world!']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Hello,  world!\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello,', '', 'world!']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Hello,  world!\".split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['www', 'networksciencelab', 'com']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"www.networksciencelab.com\".split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alpha, bravo, charlie, delta'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\", \".join([\"alpha\", \"bravo\", \"charlie\", \"delta\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1-617-305-1985'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"-\".join(\"1.617.305.1985\".split(\".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This string has many spaces'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(\"This string\\n\\r  has   many\\t\\tspaces\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Page 12\n",
    "\"www.networksciencelab.com\".find(\".com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"www.networksciencelab.com\".count(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigList = [str(i) for i in range(10000000)]\n",
    "\"abc\" in bigList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigSet = set(bigList)\n",
    "\"abc\" in bigSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'alpha', 1: 'bravo', 2: 'charlie', 3: 'delta'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = [\"alpha\", \"bravo\", \"charlie\", \"delta\"]\n",
    "dict(enumerate(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 'alpha', 'b': 'bravo', 'c': 'charlie', 'd': 'delta'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Page 14\n",
    "kseq = \"abcd\"\n",
    "vseq = [\"alpha\", \"bravo\", \"charlie\", \"delta\"]\n",
    "dict(zip(kseq, vseq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 3), ('man', 1), ('plan', 1), ('canal', 1), ('panama', 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Page 17\n",
    "from collections import Counter\n",
    "phrase = \"a man a plan a canal panama\"\n",
    "cntr = Counter(phrase.split())\n",
    "cntr.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 3, 'man': 1, 'plan': 1, 'canal': 1, 'panama': 1}\n"
     ]
    }
   ],
   "source": [
    "cntrDict = dict(cntr.most_common())\n",
    "print(cntrDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cntrDict['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page 19\n",
    "import urllib.request\n",
    "try:\n",
    "    with urllib.request.urlopen(\"http://www.networksciencelab.com\") as doc:\n",
    "        html = doc.read()\n",
    "except:\n",
    "    print(\"Could not open %s\" % doc, file=sys.err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParseResult(scheme='http', netloc='networksciencelab.com', path='/index.html', params='param', query='foo=bar', fragment='content')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Page 20\n",
    "import urllib.parse\n",
    "URL = \"http://networksciencelab.com/index.html;param?foo=bar#content\"\n",
    "urllib.parse.urlparse(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', '', 'world', '']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Page 24\n",
    "import re\n",
    "re.split(r\"\\W\", \"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'world', '']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r\"\\W+\", \"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(0, 3), match='067'>\n"
     ]
    }
   ],
   "source": [
    "mo = re.match(r\"\\d+\", \"067 Starts with a number\")\n",
    "print(mo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'067'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mo.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(re.match(r\"\\d+\", \"Does not start with a number\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(8, 11), match='Has'>\n"
     ]
    }
   ],
   "source": [
    "print(re.search(r\"[a-z]+\", \"0010010 Has at least one 010 letter 0010010\", re.I))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(9, 11), match='as'>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r\"[a-z]+\", \"0010010 Has at least one 010 letter 0010010\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Has', 'at', 'least', 'one', 'letter']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r\"[a-z]+\", \"0010010 Has at least one 010 letter 0010010\", re.I))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0010010[...]010[...]0010010'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Page 25\n",
    "re.sub(r\"[a-z ]+\", \"[...]\", \"0010010 has at least one 010 letter 0010010\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['core-python.ipynb']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Page 26\n",
    "import glob\n",
    "glob.glob(\"*.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page 27\n",
    "import pickle\n",
    "with open(\"myData.pickle\", \"wb\") as oFile:\n",
    "    pickle.dump(object, oFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"myData.pickle\", \"rb\") as iFile:\n",
    "    object = pickle.load(iFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequency Counter\n",
    "I'm downloading a url using urllib, finding all the words using regex, then count them all up with Counter.\n",
    "I'm also using sorted with a lamda to sort the words by their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a : 676\n",
      "li : 506\n",
      "class : 477\n",
      "title : 349\n",
      "href : 332\n",
      "link : 308\n",
      "interlanguage : 291\n",
      "wiki : 262\n",
      "E0 : 244\n",
      "span : 186\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "import collections\n",
    "\n",
    "url = \"http://simple.wikipedia.com/wiki/Computer_science\"\n",
    "try:\n",
    "    with urllib.request.urlopen(url) as doc:\n",
    "        html = doc.read().decode('utf-8')\n",
    "    words = re.findall(r\"\\w+\", html, re.I)\n",
    "    index = collections.Counter(words)\n",
    "    top = sorted(index.items(), key=lambda pair: pair[1], reverse=True)[:10]\n",
    "    for word in top:\n",
    "        print(\"{} : {}\".format(word[0], word[1]))\n",
    "except e:\n",
    "    print(\"Could not open %s\" % doc)\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Indexer\n",
    "I'm using glob to find the files underneath the test directory, which contains three gutenburg project books.\n",
    "I use the regex to find all the words and store the counts in a huge dict(). I have to catch the UnicodeDecodeError in case there is a binary file or non-unicode compilant file. I then save and load a pickle file and print which files contain the word \"the\". They all do, so there are three results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import glob\n",
    "import re\n",
    "directory = \"test\"\n",
    "index = dict()\n",
    "files = glob.glob(directory + \"/*\")\n",
    "for file in files:\n",
    "    try:\n",
    "        with open(file, \"r\") as infile:\n",
    "            words = set(re.findall(r\"\\w+\", infile.read(), re.I))\n",
    "            for word in words:\n",
    "                if word in index:\n",
    "                    index[word].append(file)\n",
    "                else:\n",
    "                    index[word] = [file]\n",
    "    except UnicodeDecodeError:\n",
    "        pass\n",
    "with open(\"index.dat\", \"wb\") as datafile:\n",
    "    pickle.dump(index, datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test/prideandprejudice.txt', 'test/aliceinwonderland.txt', 'test/mobydick.txt']\n"
     ]
    }
   ],
   "source": [
    "with open(\"index.dat\", \"rb\") as datafile:\n",
    "    index =pickle.load(datafile)\n",
    "    print(index[\"the\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
